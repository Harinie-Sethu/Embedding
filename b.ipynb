{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxFiRPnrApWn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, tensor, optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from numpy import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import urllib.request\n",
        "import gzip\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Vw3zjI_-A3",
        "outputId": "44774169-8150-4ded-b404-47d38e6a6761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(path):\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            yield json.loads(line)\n",
        "\n",
        "# Initialize an empty list to store the sentences\n",
        "para = []\n",
        "data_path = 'data'\n",
        "\n",
        "# Loop through the JSON data and extract the sentences\n",
        "# for review in parse(data_path):\n",
        "#     para.append(review['reviewText'])\n",
        "\n",
        "num_reviews_to_read =  1000\n",
        "\n",
        "for i, review in enumerate(parse(data_path)):\n",
        "    para.append(review['reviewText'])\n",
        "    \n",
        "    # Stop reading the dataset once the desired number of reviews have been processed\n",
        "    if i >= num_reviews_to_read - 1:\n",
        "        break\n",
        "\n",
        "        \n",
        "pattern = re.compile(r'(?<![A-Z])[.!?]\\s+')\n",
        "\n",
        "# iterate over each paragraph and split it into sentences\n",
        "sentences = []\n",
        "for paragraph in para:\n",
        "    sentences.extend(re.split(pattern, paragraph))\n",
        "sentences = [re.sub(r'[^\\w\\s]+', '', sentence.strip().lower()) for sentence in sentences]"
      ],
      "metadata": {
        "id": "N32LZqq18S2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    sent = re.sub(r'[^\\w\\s\\.\\-]+', '', sent)\n",
        "    sent = re.sub('([;:.,!?()])', r' \\1 ', sent)\n",
        "    words = sent.strip().split()  # split the sentence into a list of words\n",
        "    for word in words:\n",
        "        word_freq[word] += 1\n"
      ],
      "metadata": {
        "id": "Wad8Yq8qARb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_count = sum(1 for count in word_freq.values() if count < 5)\n",
        "word_freq['<UNK>'] = unk_count\n"
      ],
      "metadata": {
        "id": "0io_ldHXbTqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary list and replace rare words with '<UNK>'\n",
        "vocab = ['<UNK>']\n",
        "rare_words = []\n",
        "new_sentences = []\n",
        "word_counts = {}\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "    words = sent.strip().split()  # split the sentence into a list of words\n",
        "    new_sent = []\n",
        "    for word in words:\n",
        "        # remove non-alphanumeric characters from each word\n",
        "        word = re.sub(r'\\W+', '', word)\n",
        "        if word_freq[word.lower()] < 5:\n",
        "            new_sent.append('<UNK>')\n",
        "            rare_words.append(word.lower())\n",
        "        else:\n",
        "            new_sent.append(word)\n",
        "    new_sentences.append(' '.join(new_sent))\n",
        "\n",
        "# Add counts of rare words to <UNK>\n",
        "for word in rare_words:\n",
        "    if word in word_freq:\n",
        "        if '<UNK>' not in word_counts:\n",
        "            word_counts['<UNK>'] = 0\n",
        "        word_counts['<UNK>'] += word_freq[word.lower()]\n",
        "        del word_freq[word.lower()]\n"
      ],
      "metadata": {
        "id": "bsqQ9fcr8W1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_sentences[55])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTY5yCyl_ijY",
        "outputId": "b2612391-f430-44e3-f892-47c937a231c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i <UNK> through a lot of it and wouldnt use it to show to a group as i had <UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_vocab = set()\n",
        "\n",
        "for sentence in new_sentences:\n",
        "    words = sentence.split()\n",
        "    unique_vocab.update(words)\n",
        "\n",
        "unique_vocab = list(unique_vocab)\n",
        "\n",
        "vocabulary = sorted(list(vocab))\n",
        "print(len(unique_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD_wLZ4B8p71",
        "outputId": "1759bdb3-2d14-4d03-c6f8-4b65cccc9061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert unique_vocab to a dictionary with indices as values\n",
        "word_to_index = {word: i for i, word in enumerate(unique_vocab)}\n",
        "\n",
        "# Create one hot vectors for each word in the vocabulary\n",
        "one_hot_vectors = []\n",
        "for word in unique_vocab:\n",
        "    word_index = word_to_index[word]\n",
        "    one_hot_vector = F.one_hot(torch.tensor(word_index), len(unique_vocab)).float()\n",
        "    one_hot_vectors.append(one_hot_vector)\n",
        "\n",
        "# Convert one_hot_vectors to a PyTorch tensor\n",
        "one_hot_tensor = torch.stack(one_hot_vectors)\n",
        "print(one_hot_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocs2dn9XTYvS",
        "outputId": "a30afe8d-e263-4430-a992-cc036ae05ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1457, 1457])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the probabilities of each word\n",
        "word_probs = {word: count / len(unique_vocab) for word, count in word_freq.items()}"
      ],
      "metadata": {
        "id": "iz-ztCY9UEJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate negative samples for each target word\n",
        "# num_negative_samples = 5\n",
        "\n",
        "# neg_samples = {}\n",
        "# for target_word in unique_vocab:\n",
        "#     neg_samples[target_word] = []\n",
        "#     while len(neg_samples[target_word]) < num_negative_samples:\n",
        "#         # Randomly sample a word\n",
        "#         neg_word = random.choice(unique_vocab)\n",
        "#         # Skip the target word and high-frequency words\n",
        "#         if neg_word == target_word or word_probs[neg_word] > word_probs[target_word]:\n",
        "#             continue\n",
        "#         # Add the negative word to the list\n",
        "#         neg_samples[target_word].append(neg_word)\n",
        "\n"
      ],
      "metadata": {
        "id": "NGaEOqL8WO7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute the negative examples for each target word\n",
        "num_negative_samples = 5\n",
        "\n",
        "neg_samples = {}\n",
        "for target_word in unique_vocab:\n",
        "    # Get the frequency of the target word\n",
        "    freq = word_freq[target_word]\n",
        "    # Compute the unigram distribution raised to the 3/4 power\n",
        "    weights = np.power(np.array([word_freq[word] for word in unique_vocab]), 0.75)\n",
        "    weights /= np.sum(weights)\n",
        "    # Sample negative examples according to the unigram distribution\n",
        "    neg_words = np.random.choice(unique_vocab, size=num_negative_samples*freq, replace=True, p=weights)\n",
        "    neg_words = [word for word in neg_words if word != target_word][:num_negative_samples]\n",
        "    neg_samples[target_word] = neg_words\n"
      ],
      "metadata": {
        "id": "e2ATCP-EJQJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input and output matrices\n",
        "vocab_size = len(unique_vocab)\n",
        "window_size = 2\n",
        "\n",
        "\n",
        "# Create the input and output matrices\n",
        "input_matrix = []\n",
        "output_matrix = []\n",
        "for sentence in new_sentences:\n",
        "    words = sentence.split()\n",
        "    for i, target_word in enumerate(words):\n",
        "        # Ignore the target word if it is not in the vocabulary\n",
        "        if target_word not in word_to_index:\n",
        "            continue\n",
        "        # Collect the context words\n",
        "        context_words = []\n",
        "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
        "            if j != i and words[j] in word_to_index:\n",
        "                context_words.append(words[j])\n",
        "\n",
        "        # print(context_words)\n",
        "        # Ignore the target word if there are no context words\n",
        "        if not context_words:\n",
        "            continue\n",
        "        # Create one-hot encoding of the context words\n",
        "        context_onehot = np.zeros(vocab_size)\n",
        "        for context_word in context_words:\n",
        "            # print(word_to_index[context_word])\n",
        "            context_onehot[word_to_index[context_word]] = 1\n",
        "            if context_word not in word_to_index:\n",
        "                print(\"nay\")\n",
        "                continue\n",
        "        # Add the input and output vectors to the matrices\n",
        "        input_matrix.append(context_onehot)\n",
        "        output_matrix.append(word_to_index[target_word])\n",
        "\n"
      ],
      "metadata": {
        "id": "G25SvAKwasUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (word, index) in enumerate(word_to_index.items()):\n",
        "    if i < 25:\n",
        "        print(f\"{word}: {index}\")\n",
        "    else:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-vNMnUC9628",
        "outputId": "eba5f058-a462-47a6-c6bf-adbc4d646656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jr: 0\n",
            "turner: 1\n",
            "exercise: 2\n",
            "cut: 3\n",
            "foldables: 4\n",
            "pretty: 5\n",
            "consider: 6\n",
            "serious: 7\n",
            "etc: 8\n",
            "looking: 9\n",
            "pilgrims: 10\n",
            "end: 11\n",
            "based: 12\n",
            "its: 13\n",
            "choir: 14\n",
            "fan: 15\n",
            "else: 16\n",
            "went: 17\n",
            "twist: 18\n",
            "kind: 19\n",
            "treasure: 20\n",
            "face: 21\n",
            "charming: 22\n",
            "value: 23\n",
            "sung: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_matrix[1])"
      ],
      "metadata": {
        "id": "G74GWB8ca4ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f58523b-23c8-4873-a889-4440c65c2f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor(input_matrix)\n",
        "output_tensor = torch.tensor(output_matrix)\n",
        "print(input_tensor.shape)\n",
        "print(output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVubTAcqzcRP",
        "outputId": "4b16b513-1e13-4b5a-af64-851363397713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70696, 1457])\n",
            "torch.Size([70696])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_matrix[55])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YPjL86OznGp",
        "outputId": "ea889148-38e2-4d5f-a32c-a4d9e47b9c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define one-hot encoding function\n",
        "def one_hot(index, size):\n",
        "    vec = [0] * size\n",
        "    vec[index] = 1\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "eWRe1CtjDJL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        summed = torch.sum(embedded, dim=1)\n",
        "        out = self.linear(summed)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ThF-g3DEJIXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "embedding_size = 100\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "#also includes window_size, vocab_size, num_negative_Samples\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CBOWModel(vocab_size, embedding_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "NLZYyMfcJTtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i in range(len(input_matrix)):\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        inputs = torch.tensor(input_matrix[i]).unsqueeze(0)\n",
        "        inputs = torch.tensor(input_matrix[i]).unsqueeze(0).long()\n",
        "        target = torch.tensor(output_matrix[i]).unsqueeze(0)\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update running loss\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(input_matrix)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0cpOtOLJmU1",
        "outputId": "97826d27-51cb-4700-8420-c325d3a05daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: nan\n",
            "Epoch 2, Loss: nan\n",
            "Epoch 3, Loss: nan\n",
            "Epoch 4, Loss: nan\n",
            "Epoch 5, Loss: nan\n",
            "Epoch 6, Loss: nan\n",
            "Epoch 7, Loss: nan\n",
            "Epoch 8, Loss: nan\n",
            "Epoch 9, Loss: nan\n",
            "Epoch 10, Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the embeddings from the model\n",
        "# embeddings = model.embedding.weight.detach().numpy()\n",
        "# # Convert embeddings to a dictionary\n",
        "# embedding_dict = {f\"embedding_{i}\": embedding.tolist() for i, embedding in enumerate(embeddings)}\n",
        "\n",
        "# # # Save the embeddings to a file\n",
        "# # np.savetxt(\"cbow_emb.txt\", embeddings)\n",
        "# filename = 'cbow_emb.txt'\n",
        "\n",
        "# # Open the file for writing\n",
        "# with open(filename, 'w') as f:\n",
        "#     # Write the contents of emb_list to the file\n",
        "#     for key, value in embedding_dict.items():\n",
        "#         f.write(f\"{key}: {value}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "D2args3OIhkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embeddings from the model\n",
        "embeddings = model.embedding.weight.detach().numpy()\n",
        "\n",
        "# Convert embeddings to a dictionary with vocab as keys\n",
        "embedding_dict = {unique_vocab[i]: embeddings[i].tolist() for i in range(len(unique_vocab))}\n",
        "\n",
        "# Save the embeddings to a file\n",
        "filename = 'cbow.txt'\n",
        "\n",
        "# Open the file for writing\n",
        "with open(filename, 'w') as f:\n",
        "    # Write the contents of embedding_dict to the file\n",
        "    for key, value in embedding_dict.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "y1WdV4fUOXxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}